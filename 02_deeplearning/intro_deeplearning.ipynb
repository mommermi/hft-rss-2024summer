{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lYpB4sFpM4D"
      },
      "source": [
        "# Introduction to Deep Learning for Remote Sensing with Pytorch\n",
        "\n",
        "*HFT Stuttgart, RSS, 2024 Summer Term, Michael Mommert (michael.mommert@hft-stuttgart.de)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jbNYaYZpM4D"
      },
      "source": [
        "This notebook serves as a tutorial for how to use Pytorch for Deep Learning in a remote sensing setting. In this tutorial, we will introduce the supervised learning pipeline with the goal to train a Neural Network that performs climate zone classification based on Sentinel-2 imagery and Sentinel-1 SAR data. You can build upon the code presented in this tuturiol in your projects. Model architectures and datasets can be easily exchanged and modified."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ_lkuhgpM4F"
      },
      "source": [
        "## Content\n",
        "\n",
        "1. [Setting up the environment](#setup)\n",
        "2. [Data Inspection](#data_inspection)\n",
        "3. [Data Handling](#data)\n",
        "4. [Model Implementation](#model)\n",
        "5. [Training and Validation Pipeline](#train-val)\n",
        "6. [Hyperparameter Tuning](#hyperpars)\n",
        "7. [Evaluation](#evaluation)\n",
        "8. [Inference](#inference)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_C3eI-7pM4F"
      },
      "source": [
        "<a id='setup'></a>\n",
        "## 1. Setup\n",
        "\n",
        "We're setting up our Python environment for this tutorial by installing and importing the necessary modules and packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wi5NrxiUYT_m"
      },
      "outputs": [],
      "source": [
        "# system level modules for handling files and file structures\n",
        "import os\n",
        "import tarfile\n",
        "import copy\n",
        "\n",
        "# scipy ecosystem imports for numerics, data handling and plotting\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# pytorch and helper modules\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision.models import resnet18\n",
        "try:\n",
        "    from torchmetrics import Accuracy\n",
        "except ImportError:\n",
        "    !pip install torchmetrics\n",
        "    from torchmetrics import Accuracy\n",
        "\n",
        "# utils\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "\n",
        "# rasterio for reading in satellite image data\n",
        "try:\n",
        "    import rasterio as rio\n",
        "except ImportError:\n",
        "    !pip install rasterio\n",
        "    import rasterio as rio\n",
        "from rasterio.enums import Resampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faebKUOu5anG"
      },
      "source": [
        "We download the **ben-ge-800** dataset, if not yet present:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObqRQubiO_rA"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('ben-ge-800.tar.gz'):\n",
        "    !gdown 173P0mATJR6LVEKI74lPEhz9oUTK4sWDv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvTJ9WPQOxon"
      },
      "source": [
        "**ben-ge-800** contains samples for 800 locations with co-located Sentinel-1 SAR data, Sentinel-2 multispectral data, elevation data, land-use/land-cover data, as well as environmental data. **ben-ge-800** is a subset of the much larger **ben-ge** dataset (see [https://github.com/HSG-AIML/ben-ge](https://github.com/HSG-AIML/ben-ge) for details.) We deliberately use a very small subset of **ben-ge** to enable reasonable runtimes for the examples shown in this tutorial.\n",
        "\n",
        "We extract the `ben-ge-800.tar.gz` archive. To this end, we use the [tarfile](https://docs.python.org/3.6/library/tarfile.html) module from the Python standard library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0hu6E-NPGoH"
      },
      "outputs": [],
      "source": [
        "tar_path = os.path.join('ben-ge-800.tar.gz')\n",
        "data_base_path = os.path.abspath('.')\n",
        "\n",
        "with tarfile.open(tar_path, mode='r') as tar:\n",
        "    tar.extractall(path=data_base_path)\n",
        "\n",
        "data_base_path = os.path.join(data_base_path, 'ben-ge-800')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yver4nem5anJ"
      },
      "source": [
        "The environment is set up and the data in place. Before we define the dataset classes and dataloaders to access the data efficiently, we fix some random seeds to obtain reproduceable results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOZcKSCT5anJ"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)     # sets the seed value in Numpy\n",
        "torch.manual_seed(42)  # sets the seed value in Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4UnJK1N5anJ"
      },
      "source": [
        "<a id='data_inspection'></a>\n",
        "## 2. Data Inspection\n",
        "\n",
        "Before we start implementing our model, let's have a look at the data. For this tutorial, we need three different data products that are available for every single sample in the dataset:\n",
        "* Sentinel-2 multispectral data: 12-band Level-2A images of size 120x120; this will be first component of the model input data\n",
        "* Sentinel-1 SAR polarization data: VH and VV polarization data of size 120x120; this will be second component of the model input data\n",
        "* [ESAWorldCover](https://esa-worldcover.org/en) land-use/land-cover labels: for each sample, a value is provided that corresponds to the percentage at which each of XXX classes covers the map of this sample; in this tutorial, we will only consider the majority class and train a Neural Network that predicts this majority class from the input consisting of Sentinel-1 and Sentinel-2 data.\n",
        "\n",
        "Let's have a look at how to access the different data products:\n",
        "\n",
        "### Sentinel-2\n",
        "\n",
        "Sentinel-2 data are located in the `ben-ge-800/sentinel-2/` directory. Each sample has its own subdirectory; random sample is named `S2B_MSIL2A_20170814T100029_90_11`. For each sample, 12 `.tif` files are available, one for each band. Let's open the red band data for this sample:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bUfVR8w5anK"
      },
      "outputs": [],
      "source": [
        "dataset = rio.open(\"ben-ge-800/sentinel-2/S2B_MSIL2A_20170814T100029_90_11/S2B_MSIL2A_20170814T100029_90_11_B04.tif\")  # open tif file with rasterio\n",
        "data = dataset.read(1)  # read data (band 1, since there is only one band)\n",
        "data.shape, data.dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ix76S6xr5anK"
      },
      "source": [
        "The data has indeed the shape 120x120 pixels (per band) and the data is stored as 16-bit integer values. Let's plot the data as a greyscale image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvXtYyHM5anK"
      },
      "outputs": [],
      "source": [
        "plt.imshow((data-np.min(data))/(np.max(data)-np.min(data)), cmap='Greys_r')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtnpvFdC5anK"
      },
      "source": [
        "To plot a color image, we have to also open and read the blue and green bands. This is straightforward, since R, G, and B have the same spatial resolutions. Most other bands, however, have different spatial resolutions. Therefore, we have to resample the data to 10m resolution. Let's do this for all bands and then plot the true color information:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lewvc8RC5anK"
      },
      "outputs": [],
      "source": [
        "# an ordered list of all the bands to be extracted\n",
        "s2_bands = [\"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B09\", \"B11\", \"B12\", \"B8A\"]\n",
        "\n",
        "# this dictionary describes the resampling factors between the individual bands\n",
        "s2_resampling_factors = {\"B01\": 6, \"B02\": 1, \"B03\": 1, \"B04\": 1, \"B05\": 2, \"B06\": 2, \"B07\": 2, \"B08\": 1, \"B09\": 6, \"B11\": 2, \"B12\": 2, \"B8A\": 2}\n",
        "\n",
        "# read all bands for one sample\n",
        "img = np.empty((12, 120, 120))\n",
        "for i, band in enumerate(s2_bands):\n",
        "    upscale_factor = s2_resampling_factors[band]\n",
        "\n",
        "    # read corresponding data file and upsample based on resampling factor\n",
        "    with rio.open(f\"ben-ge-800/sentinel-2/S2B_MSIL2A_20170814T100029_90_11/S2B_MSIL2A_20170814T100029_90_11_{band}.tif\") as dataset:\n",
        "                data = dataset.read(out_shape=(dataset.count, int(dataset.height * upscale_factor), int(dataset.width * upscale_factor)),\n",
        "                                    resampling=Resampling.bilinear)  # we can resample the bands upon reading them\n",
        "    img[i,:,:] = data\n",
        "\n",
        "# plot the RGB information for that sample\n",
        "img_rgb = np.dstack(img[1:4][::-1])  # extract RGB, reorder, and perform a deep stack (shape: 120, 120, 3)\n",
        "plt.imshow((img_rgb-np.min(img_rgb))/(np.max(img_rgb)-np.min(img_rgb)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkNgn5yV5anK"
      },
      "source": [
        "In order to normalize our Sentinel-2 data, we simply divide the pixel values in each band by 10000 and clip the range from 0 to 1. This provides a reasonable value in each band:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyCy5gMP5anK"
      },
      "outputs": [],
      "source": [
        "np.average(np.clip(img/10000, 0, 1), axis=(1,2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnW356ZC5anK"
      },
      "source": [
        "### Sentinel-1\n",
        "\n",
        "Reading Sentinel-1 data works very similar to Sentinel-2: data is stored in separate `.tif` files. Since both polarizations have the same spatial resolution, no resampling is necessary. Let's pick a random sample and plot its Sentinel-1 data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpheO5t25anK"
      },
      "outputs": [],
      "source": [
        "# an ordered list of all the bands to be extracted\n",
        "s1_bands = [\"VH\", \"VV\"]\n",
        "\n",
        "# read all Sentinel-1 bands for a random sample\n",
        "img = np.empty((2, 120, 120))\n",
        "for i, band in enumerate(s1_bands):\n",
        "    with rio.open(f\"ben-ge-800/sentinel-1/S1B_IW_GRDH_1SDV_20180327T183408_29SNB_74_62/S1B_IW_GRDH_1SDV_20180327T183408_29SNB_74_62_{band}.tif\") as dataset:\n",
        "        data = dataset.read()\n",
        "        img[i,:,:] = data\n",
        "\n",
        "# plot both polarizations for that sample\n",
        "f, ax = plt.subplots(1, 2, sharex=True, sharey=True)\n",
        "\n",
        "ax[0].imshow((img[0]-np.min(img[0]))/(np.max(img[0])-np.min(img[0])), cmap='Greys_r')\n",
        "ax[1].imshow((img[1]-np.min(img[1]))/(np.max(img[1])-np.min(img[1])), cmap='Greys_r')\n",
        "ax[0].set_title('VH')\n",
        "ax[1].set_title('VV')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDvlWxgc5anK"
      },
      "source": [
        "We normalize our Sentinel-1 data by clipping the value provided, shifting and scaling them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8he7hMii5anK"
      },
      "outputs": [],
      "source": [
        "np.average((np.clip(img, a_min=-25, a_max=0) + 25) / 25, axis=(1,2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hr4YIMdq5anL"
      },
      "source": [
        "### Land-use/land-cover data\n",
        "\n",
        "Finally, we will read in the ESAWorldCover land-use/land-cover data. Since we are only interested in the coverage of each class, the data is stored in a simple `.csv` file. We can read this file with the Pandas module:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hd7_ZDA45anL"
      },
      "outputs": [],
      "source": [
        "ewc_labels = pd.read_csv(f\"ben-ge-800/ben-ge-800_esaworldcover.csv\")\n",
        "ewc_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkiVYtqX5anL"
      },
      "source": [
        "We can display the coverage of each class across the entire dataset as a pie chart:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPyfIpSL5anM"
      },
      "outputs": [],
      "source": [
        "# we extract the different class labels\n",
        "ewc_classes = ewc_labels.columns[2:]\n",
        "\n",
        "# we sum up the coverages for each class\n",
        "ewc_coverage = []\n",
        "for c in ewc_classes:\n",
        "    ewc_coverage.append(np.sum(ewc_labels.loc[:, c]))\n",
        "ewc_coverage = np.array(ewc_coverage)/len(ewc_labels)\n",
        "\n",
        "# plot coverages\n",
        "plt.pie(ewc_coverage, labels=ewc_classes)\n",
        "print('all classes: ', ewc_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_p0XAto_5anM"
      },
      "source": [
        "In our model training, we will use only the majority class. Let's derive the majority class and recreate the pie chart:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQ_KAHQ35anM"
      },
      "outputs": [],
      "source": [
        "# derive the index of the majority class for each sample\n",
        "ewc_major = np.argmax(ewc_labels.loc[:, 'tree_cover':].values, axis=1)\n",
        "\n",
        "# redo pie chart with majority classes\n",
        "plt.pie([np.sum(ewc_major == i) for i in range(len(ewc_classes))], labels=ewc_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-le0XROa95M"
      },
      "source": [
        "<a id='data'></a>\n",
        "## 3. Data Handling\n",
        "\n",
        "In the following, we implement a dataset class that combines the data access methods introduced above for the three data modalities. The dataset class provides easy and homogeneous access to the data on a per-sample basis. The `__getitem__` method returns a simple dictionary with the different data modalities that we use for data input and as target in the training and evaluation processes. As you will see later, the dataset class will be leveraged later by data loaders that will efficiently prepare data for the training pipeline.\n",
        "\n",
        "As part of the dataset class, we apply data normalizations and output all numeric features as *Pytorch* tensors; tensors are the Pytorch equivalent of Numpy arrays but tensors can use GPU infrastructure for more efficient computations. Finally, we use predefined dataset splits: we can generate a training, validation and test dataset.\n",
        "\n",
        "Please mind that we will use only those 4 bands from Sentinel-2 with a native resolution of 10~m (B, G, R, NIR). This is done to reduce the computational load and due to the fact that the other bands contribute only little information in this case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdVdNb2qP7OQ"
      },
      "outputs": [],
      "source": [
        "class BENGE(Dataset):\n",
        "    \"\"\"A dataset class implementing the Sentinel-1, Sentinel-2 and ESAWorldCover data modalities.\"\"\"\n",
        "    def __init__(self, data_dir=None, split='train',\n",
        "                 s2_bands=[\"B02\", \"B03\", \"B04\", \"B05\"], s1_bands=[\"VH\", \"VV\"]):\n",
        "        \"\"\"Dataset class constructor\n",
        "\n",
        "        keyword arguments:\n",
        "        data_dir -- string containing the path to the base directory of ben-ge dataset, default: ben-ge-800 directory\n",
        "        split    -- string, describes the split to be instantiated, either `train`, `val` or `test`\n",
        "        s2_bands -- list of Sentinel-2 bands to be extracted, default: all bands\n",
        "        s1_bands -- list of Senintel-1 bands to be extracted, default: all bands\n",
        "\n",
        "        returns:\n",
        "        BENGE object\n",
        "        \"\"\"\n",
        "        super(BENGE, self).__init__()\n",
        "\n",
        "        # store some definitions\n",
        "        if data_dir is None:\n",
        "            self.data_dir = data_base_path\n",
        "        else:\n",
        "            self.data_dir = data_dir\n",
        "        self.s2_bands = s2_bands\n",
        "        self.s1_bands = s1_bands\n",
        "\n",
        "        # read in relevant data files and definitions\n",
        "        self.name = self.data_dir.split(\"/\")[-1]\n",
        "        self.split = split\n",
        "        self.meta = pd.read_csv(f\"{self.data_dir}/{self.name}_meta.csv\")\n",
        "        self.meta = self.meta.loc[self.meta.split == split, :]  # filter by split\n",
        "        self.ewc_labels = pd.read_csv(f\"{self.data_dir}/{self.name}_esaworldcover.csv\")\n",
        "        self.ewc_label_names = [\"tree_cover\", \"shrubland\", \"grassland\", \"cropland\", \"built-up\",\n",
        "                                \"bare/sparse_vegetation\", \"snow_and_ice\",\"permanent_water_bodies\",\n",
        "                                \"herbaceous_wetland\", \"mangroves\",\"moss_and_lichen\"]\n",
        "        self.s2_resampling_factors = {\"B01\": 6, \"B02\": 1, \"B03\": 1, \"B04\": 1, \"B05\": 2, \"B06\": 2, \"B07\": 2, \"B08\": 1, \"B09\": 6, \"B11\": 2, \"B12\": 2, \"B8A\": 2}\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Return sample `idx` as dictionary from the dataset.\"\"\"\n",
        "        sample_info = self.meta.iloc[idx]\n",
        "        patch_id = sample_info.patch_id  # extract Sentinel-2 patch id\n",
        "        patch_id_s1 = sample_info.patch_id_s1  # extract Sentinel-1 patch id\n",
        "\n",
        "        # retrieving Sentinel-2 data\n",
        "        s2 = self.load_s2(patch_id).astype(float)  # load Sentinel-2 data\n",
        "        #s2 = np.moveaxis(s2, 0, -1)  # reordering data\n",
        "        s2 = np.clip(s2 / 10000, 0, 1)  # normalize Sentinel-2 data\n",
        "\n",
        "        # retrieving Sentinel-1 data\n",
        "        if self.s1_bands:\n",
        "            s1 = self.load_s1(patch_id_s1).astype(float)  # load Sentinel-1 data\n",
        "            s1 = (np.clip(s1, a_min=-25, a_max=0) + 25) / 25\n",
        "        else:\n",
        "            s1 = None\n",
        "\n",
        "        # extract top land-use/land-cover label\n",
        "        ewc_label = np.argmax(self.ewc_labels[self.ewc_labels.patch_id == patch_id][self.ewc_label_names])\n",
        "\n",
        "        # create sample dictionary containing all the data\n",
        "        sample = {\n",
        "            \"patch_id\": patch_id,\n",
        "            \"s2\": torch.from_numpy(s2).float(),\n",
        "            \"s1\": torch.from_numpy(s1).float(),\n",
        "            \"lulc\": torch.from_numpy(np.array([ewc_label.copy()], dtype=float)).long(),\n",
        "            }\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return length of this dataset.\"\"\"\n",
        "        return self.meta.shape[0]\n",
        "\n",
        "    def load_s2(self, patch_id):\n",
        "        \"\"\"Helper function to load Sentinel-2 data for a given `patch_id`.\"\"\"\n",
        "        img = []\n",
        "\n",
        "        for band in self.s2_bands:\n",
        "            upscale_factor = self.s2_resampling_factors.get(band)\n",
        "            # read corresponding data file and upsample based on resampling factor\n",
        "            with rio.open(f\"{self.data_dir}/sentinel-2/{patch_id}/{patch_id}_{band}.tif\") as d:\n",
        "                data = d.read(\n",
        "                out_shape=(\n",
        "                    d.count,\n",
        "                    int(d.height * upscale_factor),\n",
        "                    int(d.width * upscale_factor)\n",
        "                ),\n",
        "                resampling=Resampling.bilinear\n",
        "            )\n",
        "            img.append(data)\n",
        "\n",
        "        img = np.concatenate(img)\n",
        "        return img\n",
        "\n",
        "    def load_s1(self, s1_patch_id):\n",
        "        \"\"\"Helper function to load Sentinel-1 data for a given `patch_id`.\"\"\"\n",
        "        img = []\n",
        "\n",
        "        for band in self.s1_bands:\n",
        "            # read corresponding data file\n",
        "            with rio.open(f\"{self.data_dir}/sentinel-1/{s1_patch_id}/{s1_patch_id}_{band}.tif\") as d:\n",
        "                data = d.read()\n",
        "                img.append(data)\n",
        "\n",
        "        img = np.concatenate(img)\n",
        "        return img\n",
        "\n",
        "    def load_ewc(self, patch_id):\n",
        "        \"\"\"Helper function to load ESAWorldCover data for a given `patch_id`.\"\"\"\n",
        "        with rio.open(f\"{self.data_dir}/esaworldcover/{patch_id}_esaworldcover.tif\") as d:\n",
        "            data = d.read()\n",
        "\n",
        "        return data\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XShJZLzc5anM"
      },
      "source": [
        "We can now instantiate the different splits for this dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PygqMbD05anN"
      },
      "outputs": [],
      "source": [
        "train_data = BENGE(split='train')\n",
        "val_data = BENGE(split='val')\n",
        "test_data = BENGE(split='test')\n",
        "\n",
        "len(train_data), len(val_data), len(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmzCS6cP5anN"
      },
      "source": [
        "We can retrieve a single sample simply by indexing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6iJ4PFY5anN"
      },
      "outputs": [],
      "source": [
        "train_data[12]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCfRser65anN"
      },
      "source": [
        "For Neural Network training we have to define data loaders. When we do so, we have to define the batch size, which is typically limited by the GPU RAM during training. For evaluation purposes, we can typically pick a larger batch size, since we need less memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vigNcosR5anO"
      },
      "outputs": [],
      "source": [
        "train_batchsize = 32\n",
        "eval_batchsize = 64\n",
        "\n",
        "train_dataloader = DataLoader(train_data, batch_size=train_batchsize, num_workers=4, pin_memory=True)\n",
        "val_dataloader = DataLoader(val_data, batch_size=eval_batchsize, num_workers=4, pin_memory=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=eval_batchsize, num_workers=4, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8_bVEfRpM4W"
      },
      "source": [
        "<a id='model'></a>\n",
        "## 4. Model Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOK06D2caPjB"
      },
      "source": [
        "In this tutorial, we will use a simple ResNet-18 model ([He et al. (2015)](https://arxiv.org/abs/1512.03385) to learn the task of patch-wise multi-class classification. This architecture is readily available through `torchvision`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaJLON_ckFW_"
      },
      "outputs": [],
      "source": [
        "model = resnet18()\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wcxm--S5anO"
      },
      "source": [
        "Now, we have to change this model slightly to use it for our task:\n",
        "* The model expects as input a tensor with 3 channels; our data, however, contains a total of 6 channels (4 channels from Sentinel-2 + 2 channels from Sentinel-1). Therefore, we have to replace the first convolutional layer to be able to use the 6 bands. Since we have to do this, we also change the kernel size to 3 to be more susceptible to small-scale features.\n",
        "* The current model implementation generates a total of 1000 output features, since it is designed for a classification problem with 1000 different classes. Since our task only distinguishes between 12 different classes, we have to modify this output layer accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xm1HYSr5anO"
      },
      "outputs": [],
      "source": [
        "model.conv1 = nn.Conv2d(6, 64, kernel_size=3, stride=1, padding=1)\n",
        "model.fc = nn.Linear(512, 12, bias=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7C9XkRKh5anP"
      },
      "source": [
        "This concludes our model implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zy_LH3aUpM4Z"
      },
      "source": [
        "<a id='train-val'></a>\n",
        "## 5. Training and Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F66Xr5Xy5anP"
      },
      "source": [
        "First of all, let's verify if a GPU is available on our compute machine. If not, the CPU will be used instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xb_5oxnbcId"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print('Device used: {}'.format(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gl4509B-8kIZ"
      },
      "source": [
        "Before we can implement the training pipeline we have to define two more things: a Loss function and an optimizer that will update our model weights during training. We also define our evaluation metric, for which we use the accuracy score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJvU_eEr5anP"
      },
      "outputs": [],
      "source": [
        "# we will use the cross entropy loss\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# we will use the Adam optimizer\n",
        "learning_rate = 0.001\n",
        "opt = optim.Adam(params=model.parameters(), lr=learning_rate)\n",
        "\n",
        "# we instantiate the accuracy metric\n",
        "accuracy = Accuracy(task=\"multiclass\", num_classes=12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mxynwEl5anP"
      },
      "source": [
        "Now, we have to move the model and the loss function on the GPU, since the computationally heavy work will be conducted there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAXnEvAu5anP"
      },
      "outputs": [],
      "source": [
        "model.to(device)\n",
        "loss.to(device)\n",
        "accuracy.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEbPAZwn5anQ"
      },
      "source": [
        "Finally, we can implement our training pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wn8Kiqd-iR3l"
      },
      "outputs": [],
      "source": [
        "epochs = 5\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accs = []\n",
        "val_accs = []\n",
        "\n",
        "for ep in range(epochs):\n",
        "\n",
        "    # we perform training for one epoch\n",
        "    model.train()   # it is very important to put your model into training mode!\n",
        "    for samples in tqdm(train_dataloader):\n",
        "        # we extract the input data (Sentinel-1 and Sentinel-2)...\n",
        "        s1 = samples['s1']\n",
        "        s2 = samples['s2']\n",
        "        # ... concatenate them along the channel axis and move them to the gpu\n",
        "        x = torch.concat([s1, s2], dim=1).to(device)\n",
        "        # x is of shape [batchsize, channels, height, width]\n",
        "\n",
        "        # now we extract the target (lulc class) and move it to the gpu\n",
        "        y = samples['lulc'].view(-1).to(device)\n",
        "\n",
        "        # we make a prediction with our model\n",
        "        output = model(x)\n",
        "\n",
        "        # we reset the graph gradients\n",
        "        model.zero_grad()\n",
        "\n",
        "        # we determine the classification loss\n",
        "        loss_val = loss(output, y)\n",
        "\n",
        "        # we run a backward pass to comput the gradients\n",
        "        loss_val.backward()\n",
        "\n",
        "        # we update the network paramaters\n",
        "        opt.step()\n",
        "\n",
        "        # we write the mini-nbatch loss and accuracy into the corresponding lists\n",
        "        train_losses.append(loss_val.detach().cpu())\n",
        "        train_accs.append(accuracy(torch.argmax(output, dim=1), y).detach().cpu())\n",
        "\n",
        "    # we evaluate the current state of the model on the validation dataset\n",
        "    model.eval()   # it is very important to put your model into evaluation mode!\n",
        "    with torch.no_grad():\n",
        "        for samples in tqdm(val_dataloader):\n",
        "            # we extract the input data (Sentinel-1 and Sentinel-2)...\n",
        "            s1 = samples['s1']\n",
        "            s2 = samples['s2']\n",
        "            # ... concatenate them along the channel axis and move them to the gpu\n",
        "            x = torch.concat([s1, s2], dim=1).to(device)\n",
        "            # x is of shape [batchsize, channels, height, width]\n",
        "\n",
        "            # now we extract the target (lulc class) and move it to the gpu\n",
        "            y = samples['lulc'].view(-1).to(device)\n",
        "\n",
        "            # we make a prediction with our model\n",
        "            output = model(x)\n",
        "\n",
        "            # we determine the classification loss\n",
        "            loss_val = loss(output, y)\n",
        "\n",
        "            # we write the mini-nbatch loss and accuracy into the corresponding lists\n",
        "            val_losses.append(loss_val.detach().cpu())\n",
        "            val_accs.append(accuracy(torch.argmax(output, dim=1), y).detach().cpu())\n",
        "\n",
        "    print(\"epoch {}: train: loss={}, acc={}; val: loss={}, acc={}\".format(\n",
        "        ep, train_losses[-1], train_accs[-1], val_losses[-1], val_accs[-1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4S81HUFy5anQ"
      },
      "source": [
        "Training progress looks good: train and validation losses are decreasing, accuracies are increasing.\n",
        "\n",
        "Let's plot the available metrics as a function of the number of training iterations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hApnOKv5anQ"
      },
      "outputs": [],
      "source": [
        "f, ax = plt.subplots(1, 2, sharex=True, figsize=(10,5))\n",
        "\n",
        "ax[0].plot(np.arange(len(train_losses))*train_batchsize, train_losses, label='Train', color='blue')\n",
        "ax[0].plot(np.arange(1, len(val_losses)+1)*len(val_data), val_losses, label='Val', color='red')\n",
        "ax[0].set_xlabel('Iterations')\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[0].legend()\n",
        "\n",
        "ax[1].plot(np.arange(len(train_accs))*train_batchsize, train_accs, label='Train', color='blue')\n",
        "ax[1].plot(np.arange(1, len(val_accs)+1)*len(val_data), val_accs, label='Val', color='red')\n",
        "ax[1].set_xlabel('Iterations')\n",
        "ax[1].set_ylabel('Accuracy')\n",
        "ax[1].legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2x-UByx5anQ"
      },
      "source": [
        "We see how the losses drop and the accuracy increases for both the training and validation dataset. Since the validation metrics follow the training metrics, we do not see any obvious signs for overfitting. Based on the validation dataset, we reach an accuracy of 68% for our multi-class classification problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyfLhN0O5anR"
      },
      "source": [
        "<a id='hyperpars'></a>\n",
        "## 6. Hyperparameter Tuning\n",
        "\n",
        "We trained the model successfully. But did we manage to get the best possible result?\n",
        "\n",
        "In order to find out, we have to perform hyperparameter tuning. The only obvious hyperparameter of our model is the learning rate. Architectural considerations can also be considered as hyperparameters, but we will keep our architecture fixed.\n",
        "\n",
        "As a result, we only have to tune the learning rate. We could try other learning rate values, such as 0.01 or 0.0001, or use a scheduler that modifies the learning rate as a function of the training progress.\n",
        "\n",
        "However, we will skip this tuning process here to save some time. Instead, we will use the trained model ``as is''.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsjgpC5d5anR"
      },
      "source": [
        "<a id='evaluation'></a>\n",
        "## 7. Evaluation\n",
        "\n",
        "After finishing the hyperparameter tuning, we can perform the final evaluation of our model. We will again use the accuracy metric, but could easily replace it or add additional metrics.\n",
        "\n",
        "To properly evaluate our model we must use the test dataset in the evaluation process. Since the model has never seen the test dataset before, it will provide a realistic estimate of the performance of the model on previously unseen data.\n",
        "\n",
        "The evaluation uses more or less the same code that we used to evaluate our model during training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k18Zn4Ff5anR"
      },
      "outputs": [],
      "source": [
        "test_accs = []\n",
        "predictions = []\n",
        "groundtruths = []\n",
        "\n",
        "model.eval()   # it is very important to put your model into evaluation mode!\n",
        "with torch.no_grad():\n",
        "    for samples in tqdm(test_dataloader):\n",
        "        # we extract the input data (Sentinel-1 and Sentinel-2)...\n",
        "        s1 = samples['s1']\n",
        "        s2 = samples['s2']\n",
        "        # ... concatenate them along the channel axis and move them to the gpu\n",
        "        x = torch.concat([s1, s2], dim=1).to(device)\n",
        "        # x is of shape [batchsize, channels, height, width]\n",
        "\n",
        "        # now we extract the target (lulc class) and move it to the gpu\n",
        "        y = samples['lulc'].view(-1).to(device)\n",
        "        groundtruths.append(y.cpu())\n",
        "\n",
        "        # we make a prediction with our model\n",
        "        output = model(x)\n",
        "\n",
        "        predictions.append(np.argmax(output.cpu().numpy(), axis=1))\n",
        "\n",
        "        # we determine the classification loss\n",
        "        loss_val = loss(output, y)\n",
        "\n",
        "        # we write the mini-nbatch loss and accuracy into the corresponding lists\n",
        "        test_accs.append(accuracy(torch.argmax(output, dim=1), y).cpu().numpy())\n",
        "\n",
        "print('test dataset accuracy:', np.mean(test_accs))\n",
        "\n",
        "# flatten predictions and groundtruths\n",
        "predictions = np.concatenate(predictions).ravel()\n",
        "groundtruths = np.concatenate(groundtruths).ravel()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhOjslEb5anR"
      },
      "source": [
        "The test dataset accuracy is 70.5%, which is close to the validation accuracy - this is a good sign!\n",
        "\n",
        "Now, let's also have a look at the performance on a per-class basis via the confusion matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqHR2Vj35anR"
      },
      "outputs": [],
      "source": [
        "# create the confusion matrix\n",
        "cm = confusion_matrix(groundtruths, predictions)\n",
        "\n",
        "# plot the confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=ewc_classes[np.unique(groundtruths)],\n",
        "                              )\n",
        "# note that not all classes are present in the test dataset\n",
        "\n",
        "disp.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSCSfteH5anR"
      },
      "source": [
        "Looking at the confusion matrix, it seems that the model does a fairly decent job at predicting the correct class for the largest classes. However, it seems unable to identify the small classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJ8RBdwS5anS"
      },
      "source": [
        "<a id='inference'></a>\n",
        "## 8. Inference\n",
        "\n",
        "Now that our model is trained and evaluated, we can use it to predict the most common land-use/land-cover class in an image patch.\n",
        "\n",
        "Let's pick a random patch and run it through the model to perfom a prediction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2kTP7Ro5anS"
      },
      "outputs": [],
      "source": [
        "# we retrieve a random sample\n",
        "sample = test_data[42]\n",
        "\n",
        "# we prepare the Sentinel-1 and Sentinel-2 data for input into the model\n",
        "input = torch.concat((sample['s1'], sample['s2']))\n",
        "\n",
        "# we run the data through the model\n",
        "output = model(input.view(1, 6, 120, 120))  # we have to change the shape of the input\n",
        "\n",
        "# we identify that class with the strongest activation signal as our prediction\n",
        "prediction = torch.argmax(output)\n",
        "\n",
        "print('actual class:', ewc_classes[sample['lulc'].item()])\n",
        "print('prediction:', ewc_classes[prediction.item()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGFwdQFl5anS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "6e741a586f6b68b554925f4bea211b6852c45fbf3b1f1159871bb4b38b6bf4de"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}